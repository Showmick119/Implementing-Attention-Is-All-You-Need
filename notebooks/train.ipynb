{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Training a Transformer Model from Scratch\n",
        "\n",
        "This notebook demonstrates how to train a Transformer model as described in the paper \"Attention Is All You Need\". We'll use a small toy dataset for demonstration purposes, but the code can be easily adapted for real translation tasks.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages and import dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch numpy tqdm sentencepiece matplotlib\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "notebook_path = Path.cwd()\n",
        "project_root = notebook_path.parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformer.config import TransformerConfig\n",
        "from transformer.model import Transformer\n",
        "from transformer.loss import masked_loss\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create a Toy Dataset\n",
        "\n",
        "For this demonstration, we'll create a simple toy dataset. In a real application, you would replace this with your actual translation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyDataset(Dataset):\n",
        "    \"\"\"Simple toy dataset for demonstration.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_samples=1000, seq_len=10, vocab_size=1000):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Generate random sequences\n",
        "        self.src_data = torch.randint(\n",
        "            4, vocab_size, (num_samples, seq_len))  # Start from 4 to reserve special tokens\n",
        "        self.tgt_data = torch.randint(\n",
        "            4, vocab_size, (num_samples, seq_len))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = ToyDataset()\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Dataset size:\", len(train_dataset))\n",
        "print(\"Number of batches:\", len(train_dataloader))\n",
        "\n",
        "# Show a sample batch\n",
        "src_batch, tgt_batch = next(iter(train_dataloader))\n",
        "print(\"\\nSample batch shapes:\")\n",
        "print(\"Source:\", src_batch.shape)\n",
        "print(\"Target:\", tgt_batch.shape)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Configuration\n",
        "\n",
        "Let's create a configuration for our Transformer model. We'll use smaller dimensions than the paper for faster training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model configuration\n",
        "config = TransformerConfig(\n",
        "    vocab_size=1000,\n",
        "    max_seq_len=10,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=3,\n",
        "    d_ff=512,\n",
        "    dropout=0.1,\n",
        "    label_smoothing=0.1,\n",
        "    max_lr=1e-3,\n",
        "    warmup_steps=1000\n",
        ")\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "for key, value in config.__dict__.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create and Initialize Model\n",
        "\n",
        "Now we'll create the Transformer model and set up the optimizer and learning rate scheduler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "model = Transformer(config).to(device)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.max_lr,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "# Create learning rate scheduler\n",
        "def lr_lambda(step):\n",
        "    # Linear warmup followed by inverse square root decay\n",
        "    if step < config.warmup_steps:\n",
        "        return float(step) / float(max(1, config.warmup_steps))\n",
        "    return float(config.warmup_steps ** 0.5) / float(step ** 0.5)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Print model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Now we'll train the model for a few epochs. We'll track the loss and learning rate to visualize the training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training settings\n",
        "num_epochs = 5\n",
        "log_interval = 10\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'loss': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "        for batch_idx, (src, tgt) in enumerate(pbar):\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "            \n",
        "            # Create target input and output\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            \n",
        "            # Forward pass\n",
        "            logits, _ = model(src, tgt_input)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = masked_loss(\n",
        "                logits,\n",
        "                tgt_output,\n",
        "                config.pad_token_id,\n",
        "                config.label_smoothing\n",
        "            )\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Update history\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            history['loss'].append(loss.item())\n",
        "            history['lr'].append(current_lr)\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'lr': f'{current_lr:.6f}'\n",
        "            })\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Visualization\n",
        "\n",
        "Let's plot the loss and learning rate curves to visualize the training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(history['loss'])\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.set_xlabel('Batch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot learning rate\n",
        "ax2.plot(history['lr'])\n",
        "ax2.set_title('Learning Rate')\n",
        "ax2.set_xlabel('Batch')\n",
        "ax2.set_ylabel('Learning Rate')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Save Model\n",
        "\n",
        "Finally, let's save the trained model and its configuration for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = project_root / 'checkpoints'\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save checkpoint\n",
        "checkpoint_path = output_dir / 'transformer_model.pt'\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'config': config.__dict__,\n",
        "    'loss': history['loss'][-1]\n",
        "}, checkpoint_path)\n",
        "\n",
        "print(f\"Model saved to {checkpoint_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
