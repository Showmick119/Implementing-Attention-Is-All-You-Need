{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Showmick119/Implementing-Attention-Is-All-You-Need/blob/main/notebooks/transformer_inference.ipynb)\n",
    "\n",
    "\n",
    "# üîÆ Transformer Inference & Testing\n",
    "\n",
    "Welcome to the inference notebook! Here we'll load our trained Transformer model and use it for English-Italian translation. This notebook demonstrates how to:\n",
    "\n",
    "## üéØ What We'll Do\n",
    "\n",
    "1. **Load Trained Model** - Import a pre-trained Transformer checkpoint\n",
    "2. **Prepare Test Data** - Set up tokenizers and test sentences\n",
    "3. **Perform Translation** - Generate Italian translations from English text\n",
    "4. **Evaluate Results** - Analyze translation quality and BLEU scores\n",
    "5. **Visualize Attention** - Explore what the model learned through attention patterns\n",
    "6. **Interactive Testing** - Try your own sentences for translation\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "This notebook assumes you have:\n",
    "- A trained Transformer model (from the training notebook)\n",
    "- The same tokenizers used during training\n",
    "- Test data or sentences you want to translate\n",
    "\n",
    "Let's dive in! ‚ú®\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üì¶ Setup & Imports\n",
    "\n",
    "Let's start by installing dependencies and importing the necessary libraries for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchtext datasets tokenizers torchmetrics matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from transformer.model import build_transformer\n",
    "    from data.dataset import causal_mask\n",
    "    from config.config import get_config\n",
    "    print(\"‚úÖ Successfully imported custom modules!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure project files are available!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, latest_weights_file_path\n",
    "from train import get_model, get_ds, run_validation\n",
    "from translate import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = translate(\"Why do I need to translate this?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = translate(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
